# Avivo - AI Image Description & RAG Bot ğŸ¤–

A Telegram bot with REST API that uses **Qwen Vision Model** for image descriptions and **LangChain + FAISS** for document Q&A (RAG).

## Features

### ğŸ–¼ï¸ Vision Service (Image Analysis)
- ğŸ“¸ Image analysis using Qwen Vision Model
- ğŸ”„ Automatic image processing with caching
- ğŸ“ Detailed image descriptions
- ğŸ’¾ Redis caching for faster responses

### ğŸ“š RAG Service (Document Q&A)
- ğŸ§  **Pure LangChain implementation** - No custom code
- ğŸ—„ï¸ **FAISS vector store** for semantic search
- ğŸ“„ Multi-format document support (PDF, DOCX, TXT, XLSX, PPTX)
- ğŸ” Semantic similarity search with Ollama embeddings
- ğŸ’¬ Natural language Q&A from uploaded documents
- ğŸ’¾ Redis caching for query responses

### ğŸ›¡ï¸ Infrastructure
- âš™ï¸ Environment-based configuration
- ğŸ“Š Comprehensive logging
- ğŸŒ REST API with FastAPI
- ğŸ¤– Telegram Bot integration
- ğŸ”´ Redis caching for both services

---

## Project Structure

```
Avivo/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ core/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”‚   â””â”€â”€ logger.py              # Logging setup
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ vision_service.py      # Vision model integration
â”‚   â”‚   â”œâ”€â”€ rag_service.py         # RAG service (LangChain)
â”‚   â”‚   â”œâ”€â”€ vector_store.py        # FAISS vector store (LangChain)
â”‚   â”‚   â”œâ”€â”€ embedding_service.py   # Ollama embeddings wrapper
â”‚   â”‚   â”œâ”€â”€ document_loader.py     # LangChain document loaders
â”‚   â”‚   â””â”€â”€ cache_service.py       # Redis caching
â”‚   â”œâ”€â”€ handlers/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ telegram_handlers.py   # Bot command handlers
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ routes.py              # FastAPI routes
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ bot.py                     # Main bot class
â”‚   â””â”€â”€ app.py                     # FastAPI application
â”œâ”€â”€ temp/                          # Temporary file storage
â”œâ”€â”€ logs/                          # Application logs
â”œâ”€â”€ vector_db/                     # FAISS vector store data
â”‚   â””â”€â”€ faiss_index/
â”‚       â”œâ”€â”€ index.faiss            # FAISS index
â”‚       â””â”€â”€ index.pkl              # Document metadata
â”œâ”€â”€ main.py                        # Application entry point
â”œâ”€â”€ requirements.txt               # Python dependencies
â”œâ”€â”€ .env                           # Environment variables
â”œâ”€â”€ .env.example                   # Example environment file
â””â”€â”€ README.md                      # This file
```

---

## Installation

### 1. Prerequisites
- Python 3.8+
- Ollama installed and running
- Docker (for Redis)
- 4GB+ RAM recommended

### 2. Clone & Setup

```bash
# Clone repository
git clone <your-repo-url>
cd Avivo

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# On Windows
.venv\Scripts\activate

# On Linux/Mac
source .venv/bin/activate

# Install dependencies
pip install -r requirements.txt
```

### 3. Ollama Models

```bash
# Vision model (for image descriptions)
ollama pull qwen3-vl:4b

# Embedding model (for RAG)
ollama pull all-minilm:l6-v2

# LLM model (for RAG answer generation)
ollama pull qwen3:1.7b

# Verify models
ollama list
```

### 4. Redis Setup

```bash
# Start Redis using Docker
docker run -d --name redis-server -p 6379:6379 redis:latest

# Verify Redis is running
docker ps
```

### 5. Environment Configuration

Create `.env` file:

```env
# Telegram Bot Configuration
TELEGRAM_BOT_TOKEN=your_bot_token_here

# Ollama Configuration
OLLAMA_URL=http://localhost:11434/api/generate
OLLAMA_MODEL=qwen3-vl:4b
OLLAMA_TIMEOUT=180

# Image Processing
IMAGE_QUALITY=95
MAX_IMAGE_SIZE=10485760

# API Configuration
API_HOST=0.0.0.0
API_PORT=8000
API_TIMEOUT=180

# Redis Configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_PASSWORD=
REDIS_TTL=86400

# Embedding Model Configuration
EMBEDDING_MODEL=all-minilm:l6-v2
EMBEDDING_URL=http://localhost:11434/api/embeddings

# RAG Configuration
RAG_LLM_MODEL=qwen3:1.7b
RAG_LLM_URL=http://localhost:11434/api/generate
RAG_CHUNK_SIZE=512
RAG_CHUNK_OVERLAP=50
RAG_TOP_K=5
RAG_SIMILARITY_THRESHOLD=0.0
```

---

## Usage

### Running the Application

```bash
# Start Ollama (if not already running)
ollama serve

# Start Redis (if not already running)
docker start redis-server

# Run the application
python main.py
```

Or directly with uvicorn:

```bash
uvicorn src.app:app --reload --host 0.0.0.0 --port 8000
```

### Access Points

- **Root:** `http://localhost:8000/`
- **Health:** `http://localhost:8000/health`
- **API Docs:** `http://localhost:8000/docs` (Swagger UI)
- **ReDoc:** `http://localhost:8000/redoc`

---

## Telegram Bot Commands

### ğŸ–¼ï¸ Vision Commands
- **Send image** â†’ Get AI-powered description

### ğŸ“š RAG Commands
- **Send PDF/DOCX** â†’ Auto-upload to knowledge base
- `/ask <question>` â†’ Ask questions from knowledge base
- `/addtext <text>` â†’ Add text to knowledge base
- `/clearrag` â†’ Clear RAG knowledge base

### ğŸ“Š Other Commands
- `/start` â†’ Welcome message
- `/help` â†’ Help information
- `/stats` â†’ System statistics

### Example Usage

```
# Upload document
[User uploads PDF file: "Resume.pdf"]
Bot: âœ… Document Added Successfully!
     ğŸ“„ File: Resume.pdf
     ğŸ“š Total documents: 7 chunks

# Ask questions
/ask What is the email address?
Bot: ğŸ’¡ Answer: anishkumarmaurya12@gmail.com

# Or just ask directly (auto-detected)
What programming languages are mentioned?
Bot: ğŸ’¡ Answer: Python, JavaScript, C++, Java, SQL

# Add text knowledge
/addtext Python is a high-level programming language

# View stats
/stats
Bot: ğŸ“Š System Statistics
     ğŸ¤– Vision Model: qwen3-vl:4b
     ğŸ§  RAG Model: qwen3:1.7b
     ğŸ“š Documents: 7 chunks
```

---

## REST API Endpoints

### Vision Service

#### `POST /api/describe`
Upload image for AI description

**Request:**
```bash
curl -X POST "http://localhost:8000/api/describe" \
  -H "Content-Type: multipart/form-data" \
  -F "file=@image.jpg"
```

**Response:**
```json
{
  "success": true,
  "filename": "image.jpg",
  "description": "A detailed description...",
  "cached": false
}
```

### RAG Service

#### `POST /api/rag/upload`
Upload document to knowledge base

**Request:**
```bash
curl -X POST "http://localhost:8000/api/rag/upload" \
  -F "file=@document.pdf"
```

**Response:**
```json
{
  "success": true,
  "message": "Document 'document.pdf' added to knowledge base",
  "filename": "document.pdf"
}
```

#### `POST /api/rag/query`
Query knowledge base

**Request:**
```bash
curl -X POST "http://localhost:8000/api/rag/query?question=What+is+Python"
```

**Response:**
```json
{
  "answer": "Python is a high-level programming language...",
  "num_sources": 3
}
```

#### `DELETE /api/rag/clear`
Clear RAG knowledge base

**Response:**
```json
{
  "message": "Knowledge base cleared successfully"
}
```

#### `GET /api/rag/stats`
Get RAG system statistics

**Response:**
```json
{
  "llm_model": "qwen3:1.7b",
  "embedding_model": "all-minilm:l6-v2",
  "vector_store": {
    "total_documents": 7,
    "chunk_size": 512,
    "chunk_overlap": 50
  },
  "top_k": 5,
  "similarity_threshold": 0.0
}
```

### Cache Management

#### `DELETE /api/cache/clear?cache_type=all`
Clear Redis cache

**Parameters:**
- `cache_type`: `all`, `images`, or `rag`

**Response:**
```json
{
  "message": "Cleared all cache successfully",
  "cache_type": "all"
}
```

#### `GET /api/cache/stats`
Get cache statistics

**Response:**
```json
{
  "enabled": true,
  "total_keys": 15,
  "image_keys": 5,
  "rag_query_keys": 10,
  "used_memory": "2.5MB",
  "hits": 120,
  "misses": 30
}
```

---

## Architecture

### RAG Service Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      Telegram Bot / REST API                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       RAG Service                            â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 1. Document Loader (LangChain)                       â”‚  â”‚
â”‚  â”‚    - PyPDFLoader, Docx2txtLoader, etc.              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 2. Text Splitter (LangChain)                        â”‚  â”‚
â”‚  â”‚    - RecursiveCharacterTextSplitter                 â”‚  â”‚
â”‚  â”‚    - Chunk Size: 512, Overlap: 50                   â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 3. Embeddings (LangChain)                           â”‚  â”‚
â”‚  â”‚    - OllamaEmbeddings (all-minilm:l6-v2)           â”‚  â”‚
â”‚  â”‚    - Dimension: 384                                  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 4. Vector Store (LangChain FAISS)                   â”‚  â”‚
â”‚  â”‚    - FAISS.from_documents()                         â”‚  â”‚
â”‚  â”‚    - Semantic similarity search                      â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 5. Query Processing                                  â”‚  â”‚
â”‚  â”‚    - similarity_search_with_score()                 â”‚  â”‚
â”‚  â”‚    - Returns top-k most similar chunks              â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                         â”‚                                    â”‚
â”‚                         â–¼                                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ 6. LLM Answer Generation (LangChain)                â”‚  â”‚
â”‚  â”‚    - Ollama LLM (qwen3:1.7b)                        â”‚  â”‚
â”‚  â”‚    - PromptTemplate with context                    â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
                         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Redis Cache                               â”‚
â”‚  - Query caching (1 hour TTL)                               â”‚
â”‚  - Image description caching (24 hour TTL)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Key Technologies

- **LangChain** - Document loading, text splitting, embeddings, vector stores
- **FAISS** - Fast similarity search and clustering of dense vectors
- **Ollama** - Local LLM and embedding model hosting
- **FastAPI** - Modern REST API framework
- **python-telegram-bot** - Telegram Bot API wrapper
- **Redis** - In-memory caching

---

## Supported Document Formats

| Format | Extension | Loader |
|--------|-----------|--------|
| PDF | `.pdf` | `PyPDFLoader` |
| Word | `.docx`, `.doc` | `Docx2txtLoader` |
| Excel | `.xlsx`, `.xls` | `UnstructuredExcelLoader` |
| PowerPoint | `.pptx`, `.ppt` | `UnstructuredPowerPointLoader` |
| Text | `.txt` | `TextLoader` |

---

## Configuration

### RAG Settings

```env
# LLM for answer generation
RAG_LLM_MODEL=qwen3:1.7b

# Embedding model for semantic search
EMBEDDING_MODEL=all-minilm:l6-v2

# Chunking parameters
RAG_CHUNK_SIZE=512          # Characters per chunk
RAG_CHUNK_OVERLAP=50        # Overlap between chunks

# Search parameters
RAG_TOP_K=5                 # Number of chunks to retrieve
RAG_SIMILARITY_THRESHOLD=0.0  # Minimum similarity (0.0 = return all)
```

### Cache Settings

```env
# Redis configuration
REDIS_HOST=localhost
REDIS_PORT=6379
REDIS_DB=0
REDIS_TTL=86400  # 24 hours
```

---

## Troubleshooting

### RAG Not Responding

```bash
# Check if vector store has documents
curl http://localhost:8000/api/rag/stats

# Clear and re-upload documents
curl -X DELETE http://localhost:8000/api/rag/clear
```

### Ollama Connection Issues

```bash
# Verify Ollama is running
ollama list

# Test embedding generation
curl http://localhost:11434/api/embeddings \
  -d '{"model":"all-minilm:l6-v2","prompt":"test"}'

# Test LLM generation
curl http://localhost:11434/api/generate \
  -d '{"model":"qwen3:1.7b","prompt":"Hello","stream":false}'
```

### Redis Connection Issues

```bash
# Check Redis status
docker ps | grep redis

# Test Redis connection
redis-cli ping

# Clear all cache
curl -X DELETE "http://localhost:8000/api/cache/clear?cache_type=all"
```

### FAISS Index Corruption

```bash
# Delete vector store and restart
rm -rf vector_db/faiss_index/*
python main.py
```

---

## Performance Tips

1. **Chunk Size**: Smaller chunks (256-512) = better precision, larger (1024+) = better context
2. **Top-K**: More results = better recall but slower, fewer = faster but might miss relevant info
3. **Caching**: Enable Redis for 10-100x faster repeated queries
4. **Model Choice**:
   - **all-minilm:l6-v2**: Fast, good accuracy (384 dim)
   - **llama3:8b**: Better answers but slower (use qwen3:1.7b for speed)

---

## Requirements

```txt
python-telegram-bot==20.7
fastapi==0.109.0
uvicorn[standard]==0.27.0
langchain==0.1.0
langchain-community==0.0.20
faiss-cpu==1.8.0
redis==5.0.1
requests==2.31.0
Pillow==10.2.0
python-dotenv==1.0.0
PyPDF2==3.0.1
python-docx==1.1.0
openpyxl==3.1.2
python-pptx==0.6.23
```

---

## License

MIT License

## Author

Anish Kumar Maurya

## Contributing

Pull requests are welcome! For major changes, please open an issue first.

## Support

For issues and questions, please open an issue on GitHub.

---

## What's Next?

- [ ] Add support for images in RAG (multimodal search)
- [ ] Implement conversation history
- [ ] Add user-specific knowledge bases
- [ ] Support for more document formats (CSV, JSON, Markdown)
- [ ] Add re-ranking for better search results
- [ ] Implement streaming responses for long answers


